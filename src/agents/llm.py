from langchain_openai import ChatOpenAI
from langchain_deepseek import ChatDeepSeek
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from typing import Optional, Dict, Any
import logging

from src.config import (
    OPENAI_MODEL, OPENAI_BASE_URL, OPENAI_API_KEY,
    CLAUDE_MODEL, CLAUDE_BASE_URL, CLAUDE_API_KEY,
    GOOGLE_MODEL, GOOGLE_BASE_URL, GOOGLE_API_KEY,
    QWEN_MODEL, QWEN_BASE_URL, QWEN_API_KEY,
    DEEPSEEK_MODEL, DEEPSEEK_BASE_URL, DEEPSEEK_API_KEY,
    OLLAMA_MODEL, OLLAMA_BASE_URL, OLLAMA_API_KEY,
)
from src.config.agents import LLMProvider

logger = logging.getLogger(__name__)

# å‚å•†é…ç½®æ˜ å°„
PROVIDER_CONFIGS = {
    "openai": {
        "model": OPENAI_MODEL,
        "base_url": OPENAI_BASE_URL,
        "api_key": OPENAI_API_KEY,
        "llm_class": ChatOpenAI,
        "api_key_param": "api_key",
        "base_url_param": "base_url"
    },
    "claude": {
        "model": CLAUDE_MODEL,
        "base_url": CLAUDE_BASE_URL,
        "api_key": CLAUDE_API_KEY,
        "llm_class": ChatAnthropic,  # Claudeä½¿ç”¨ä¸“é—¨çš„Anthropicæ¥å£
        "api_key_param": "api_key",
        "base_url_param": "base_url"
    },
    "google": {
        "model": GOOGLE_MODEL,
        "base_url": GOOGLE_BASE_URL,
        "api_key": GOOGLE_API_KEY,
        "llm_class": ChatGoogleGenerativeAI,
        "api_key_param": "google_api_key",
        "base_url_param": None  # Googleä¸ä½¿ç”¨base_url
    },
    "qwen": {
        "model": QWEN_MODEL,
        "base_url": QWEN_BASE_URL,
        "api_key": QWEN_API_KEY,
        "llm_class": ChatOpenAI,  # é€šä¹‰åƒé—®ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
        "api_key_param": "api_key",
        "base_url_param": "base_url"
    },
    "deepseek": {
        "model": DEEPSEEK_MODEL,
        "base_url": DEEPSEEK_BASE_URL,
        "api_key": DEEPSEEK_API_KEY,
        "llm_class": ChatDeepSeek,
        "api_key_param": "api_key",
        "base_url_param": "api_base"  # DeepSeekä½¿ç”¨api_base
    },
    "ollama": {
        "model": OLLAMA_MODEL,
        "base_url": OLLAMA_BASE_URL,
        "api_key": OLLAMA_API_KEY,
        "llm_class": ChatOpenAI,  # Ollamaä½¿ç”¨OpenAIå…¼å®¹æ¥å£
        "api_key_param": "api_key",
        "base_url_param": "base_url"
    }
}


def create_llm_by_provider(
    provider: LLMProvider,
    temperature: float = 0.0,
    **kwargs
) -> ChatOpenAI | ChatDeepSeek | ChatGoogleGenerativeAI | ChatAnthropic:
    """
    æ ¹æ®å‚å•†åç§°åˆ›å»ºLLMå®ä¾‹
    
    Args:
        provider: å‚å•†åç§°
        temperature: æ¸©åº¦å‚æ•°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        LLMå®ä¾‹
    """
    if provider not in PROVIDER_CONFIGS:
        raise ValueError(f"ä¸æ”¯æŒçš„å‚å•†: {provider}. æ”¯æŒçš„å‚å•†: {list(PROVIDER_CONFIGS.keys())}")
    
    config = PROVIDER_CONFIGS[provider]
    llm_class = config["llm_class"]
    
    # æ„å»ºLLMå‚æ•°
    llm_kwargs = {
        "model": config["model"],
        "temperature": temperature,
        **kwargs
    }
    
    # å¤„ç†APIå¯†é’¥
    api_key_param = config["api_key_param"]
    api_key = config["api_key"]
    if api_key_param and api_key:
        llm_kwargs[api_key_param] = api_key
    
    # å¤„ç†Base URL
    base_url_param = config["base_url_param"]
    base_url = config["base_url"]
    if base_url_param and base_url:
        llm_kwargs[base_url_param] = base_url
    
    logger.info(f"åˆ›å»º {provider} LLMå®ä¾‹: {config['model']}")
    return llm_class(**llm_kwargs)


# ç¼“å­˜LLMå®ä¾‹
_llm_cache: Dict[LLMProvider, Any] = {}


def get_llm_by_provider(provider: LLMProvider) -> ChatOpenAI | ChatDeepSeek | ChatGoogleGenerativeAI | ChatAnthropic:
    """
    æ ¹æ®å‚å•†è·å–LLMå®ä¾‹ï¼Œæ”¯æŒç¼“å­˜
    
    Args:
        provider: å‚å•†åç§°
        
    Returns:
        LLMå®ä¾‹
    """
    if provider in _llm_cache:
        return _llm_cache[provider]
    
    llm = create_llm_by_provider(provider)
    _llm_cache[provider] = llm
    return llm


def get_provider_config(provider: LLMProvider) -> Dict[str, Any]:
    """
    è·å–å‚å•†é…ç½®ä¿¡æ¯
    
    Args:
        provider: å‚å•†åç§°
        
    Returns:
        å‚å•†é…ç½®ä¿¡æ¯
    """
    if provider not in PROVIDER_CONFIGS:
        raise ValueError(f"ä¸æ”¯æŒçš„å‚å•†: {provider}")
    
    config = PROVIDER_CONFIGS[provider]
    return {
        "provider": provider,
        "model": config["model"],
        "llm_class": config["llm_class"].__name__,
        "base_url": config["base_url"],
        "api_key_configured": bool(config["api_key"]),
        "api_key_param": config["api_key_param"],
        "base_url_param": config["base_url_param"]
    }


def list_supported_providers() -> Dict[str, Dict[str, Any]]:
    """
    åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„å‚å•†é…ç½®
    
    Returns:
        å‚å•†é…ç½®ä¿¡æ¯
    """
    result = {}
    for provider in PROVIDER_CONFIGS.keys():
        result[provider] = get_provider_config(provider)
    return result


# å‘åå…¼å®¹çš„å‡½æ•°
def create_openai_llm(
    model: str,
    base_url: Optional[str] = None,
    api_key: Optional[str] = None,
    temperature: float = 0.0,
    **kwargs,
) -> ChatOpenAI:
    """åˆ›å»ºOpenAI LLMå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    llm_kwargs = {"model": model, "temperature": temperature, **kwargs}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    return ChatOpenAI(**llm_kwargs)


def create_deepseek_llm(
    model: str,
    base_url: Optional[str] = None,
    api_key: Optional[str] = None,
    temperature: float = 0.0,
    **kwargs,
) -> ChatDeepSeek:
    """åˆ›å»ºDeepSeek LLMå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    llm_kwargs = {"model": model, "temperature": temperature, **kwargs}
    if base_url:
        llm_kwargs["api_base"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    return ChatDeepSeek(**llm_kwargs)


def create_google_llm(
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.0,
    **kwargs,
) -> ChatGoogleGenerativeAI:
    """åˆ›å»ºGoogle LLMå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    llm_kwargs = {"model": model, "temperature": temperature, **kwargs}
    if api_key:
        llm_kwargs["google_api_key"] = api_key
    return ChatGoogleGenerativeAI(**llm_kwargs)


if __name__ == "__main__":
    # æµ‹è¯•æ‰€æœ‰å‚å•†é…ç½®
    print("ğŸ” å‚å•†é…ç½®æµ‹è¯•:")
    providers = list_supported_providers()
    for provider, config in providers.items():
        print(f"  {provider:<10}: {config['model']:<30} | {config['llm_class']:<25} | API Key: {'âœ…' if config['api_key_configured'] else 'âŒ'}")
    
    # æµ‹è¯•LLMåˆ›å»º
    print(f"\nğŸš€ æµ‹è¯•LLMåˆ›å»º:")
    for provider in ["openai", "deepseek"]:  # åªæµ‹è¯•æœ‰é…ç½®çš„å‚å•†
        try:
            config = get_provider_config(provider)
            if config["api_key_configured"]:
                llm = get_llm_by_provider(provider)
                print(f"  âœ… {provider}: {type(llm).__name__} - {config['model']}")
            else:
                print(f"  âŒ {provider}: API Keyæœªé…ç½®")
        except Exception as e:
            print(f"  âŒ {provider}: åˆ›å»ºå¤±è´¥ - {e}")
