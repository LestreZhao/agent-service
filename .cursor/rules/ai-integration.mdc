---
description:
globs:
alwaysApply: false
---
# FusionAI AIé›†æˆæŒ‡å—

## ğŸ¤– AIæ¨¡å‹é›†æˆæ¶æ„

### æ”¯æŒçš„AIæä¾›å•†
FusionAI æ”¯æŒå¤šç§ä¸»æµ AI æ¨¡å‹æä¾›å•†ï¼Œå®ç°ç»Ÿä¸€çš„è°ƒç”¨æ¥å£ï¼š

```python
# src/agents/llm.py
from langchain_openai import ChatOpenAI
from langchain_deepseek import ChatDeepSeek
from langchain_community.llms import Ollama

class LLMManager:
    """è¯­è¨€æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.providers = {
            'openai': self._init_openai,
            'deepseek': self._init_deepseek,
            'ollama': self._init_ollama,
        }
    
    def get_llm(self, provider: str = 'openai', **kwargs):
        """è·å–æŒ‡å®šçš„è¯­è¨€æ¨¡å‹å®ä¾‹"""
        if provider not in self.providers:
            raise ValueError(f"Unsupported provider: {provider}")
        
        return self.providers[provider](**kwargs)
    
    def _init_openai(self, model: str = "gpt-4", **kwargs):
        """åˆå§‹åŒ–OpenAIæ¨¡å‹"""
        return ChatOpenAI(
            model=model,
            temperature=kwargs.get('temperature', 0.7),
            max_tokens=kwargs.get('max_tokens', 2000),
            **kwargs
        )
    
    def _init_deepseek(self, model: str = "deepseek-chat", **kwargs):
        """åˆå§‹åŒ–DeepSeekæ¨¡å‹"""
        return ChatDeepSeek(
            model=model,
            temperature=kwargs.get('temperature', 0.7),
            max_tokens=kwargs.get('max_tokens', 2000),
            **kwargs
        )
```

## ğŸ”§ æ™ºèƒ½ä»£ç†é…ç½®

### åŸºç¡€ä»£ç†ç»“æ„
```python
# src/agents/base_agent.py
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from langchain.agents import AgentExecutor
from langchain.memory import ConversationBufferMemory

class BaseAgent(ABC):
    """æ™ºèƒ½ä»£ç†åŸºç±»"""
    
    def __init__(self, name: str, description: str, llm_config: Dict = None):
        self.name = name
        self.description = description
        self.llm_config = llm_config or {}
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        self.tools = []
        self._setup_agent()
    
    @abstractmethod
    def _setup_agent(self):
        """è®¾ç½®ä»£ç†çš„å…·ä½“é…ç½®"""
        pass
    
    @abstractmethod
    async def execute(self, task: str, context: Dict = None) -> Dict:
        """æ‰§è¡Œä»»åŠ¡çš„ä¸»è¦æ–¹æ³•"""
        pass
    
    def add_tool(self, tool):
        """æ·»åŠ å·¥å…·åˆ°ä»£ç†"""
        self.tools.append(tool)
    
    def get_prompt_template(self) -> str:
        """è·å–ä»£ç†çš„æç¤ºæ¨¡æ¿"""
        return f"""
        ä½ æ˜¯ä¸€ä¸ªåä¸º {self.name} çš„AIåŠ©æ‰‹ã€‚
        ä½ çš„èŒè´£æ˜¯ï¼š{self.description}
        
        è¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚ï¼Œä½¿ç”¨å¯ç”¨çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚
        
        å¯ç”¨å·¥å…·ï¼š
        {{tools}}
        
        è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
        æ€è€ƒï¼š[åˆ†æç”¨æˆ·è¯·æ±‚å’Œé€‰æ‹©åˆé€‚çš„å·¥å…·]
        è¡ŒåŠ¨ï¼š[é€‰æ‹©è¦ä½¿ç”¨çš„å·¥å…·]
        è¡ŒåŠ¨è¾“å…¥ï¼š[å·¥å…·çš„è¾“å…¥å‚æ•°]
        è§‚å¯Ÿï¼š[å·¥å…·æ‰§è¡Œçš„ç»“æœ]
        æœ€ç»ˆç­”æ¡ˆï¼š[åŸºäºå·¥å…·ç»“æœçš„æœ€ç»ˆå›å¤]
        """
```

### ä¸“ç”¨ä»£ç†å®ç°
```python
# src/agents/research_agent.py
from .base_agent import BaseAgent
from ..tools import WebSearchTool, CrawlTool, DocumentAnalyzer

class ResearchAgent(BaseAgent):
    """ç ”ç©¶å‹æ™ºèƒ½ä»£ç†"""
    
    def __init__(self, llm_config: Dict = None):
        super().__init__(
            name="ç ”ç©¶åŠ©æ‰‹",
            description="ä¸“é—¨è´Ÿè´£ä¿¡æ¯æ”¶é›†ã€ç½‘é¡µæŠ“å–å’Œå†…å®¹åˆ†æçš„æ™ºèƒ½ä»£ç†",
            llm_config=llm_config
        )
    
    def _setup_agent(self):
        """é…ç½®ç ”ç©¶ä»£ç†çš„å·¥å…·"""
        # æ·»åŠ æœç´¢å·¥å…·
        self.add_tool(WebSearchTool())
        
        # æ·»åŠ ç½‘é¡µæŠ“å–å·¥å…·
        self.add_tool(CrawlTool())
        
        # æ·»åŠ æ–‡æ¡£åˆ†æå·¥å…·
        self.add_tool(DocumentAnalyzer())
        
        # é…ç½®ä¸“ç”¨æç¤ºæ¨¡æ¿
        self.prompt_template = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿ï¼š
        1. åœ¨çº¿ä¿¡æ¯æœç´¢å’ŒéªŒè¯
        2. ç½‘é¡µå†…å®¹æŠ“å–å’Œåˆ†æ
        3. æ–‡æ¡£å¤„ç†å’Œä¿¡æ¯æå–
        4. æ•°æ®æ•´ç†å’ŒæŠ¥å‘Šç”Ÿæˆ
        
        å½“ç”¨æˆ·æå‡ºç ”ç©¶éœ€æ±‚æ—¶ï¼Œè¯·ï¼š
        1. åˆ†æç”¨æˆ·çš„å…·ä½“éœ€æ±‚
        2. é€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œä¿¡æ¯æ”¶é›†
        3. å¯¹æ”¶é›†çš„ä¿¡æ¯è¿›è¡Œåˆ†æå’ŒéªŒè¯
        4. æä¾›ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Š
        
        å¯ç”¨å·¥å…·ï¼š{tools}
        """
    
    async def execute(self, task: str, context: Dict = None) -> Dict:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        context = context or {}
        
        # è§£æä»»åŠ¡ç±»å‹
        if "æœç´¢" in task or "æŸ¥æ‰¾" in task:
            return await self._handle_search_task(task, context)
        elif "æŠ“å–" in task or "çˆ¬å–" in task:
            return await self._handle_crawl_task(task, context)
        elif "åˆ†æ" in task:
            return await self._handle_analysis_task(task, context)
        else:
            return await self._handle_general_research(task, context)
    
    async def _handle_search_task(self, task: str, context: Dict) -> Dict:
        """å¤„ç†æœç´¢ä»»åŠ¡"""
        # å®ç°æœç´¢é€»è¾‘
        search_tool = next(tool for tool in self.tools if isinstance(tool, WebSearchTool))
        
        # æå–æœç´¢å…³é”®è¯
        keywords = self._extract_keywords(task)
        
        # æ‰§è¡Œæœç´¢
        search_results = await search_tool.search(keywords)
        
        return {
            "type": "search",
            "results": search_results,
            "summary": self._summarize_search_results(search_results)
        }
```

## ğŸ“Š LangGraphå·¥ä½œæµé›†æˆ

### å·¥ä½œæµçŠ¶æ€å®šä¹‰
```python
# src/graph/workflow_state.py
from typing import Dict, List, Optional, TypedDict
from langchain.schema import BaseMessage

class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    
    # åŸºç¡€ä¿¡æ¯
    user_input: str
    session_id: str
    
    # å¤„ç†çŠ¶æ€
    current_step: str
    completed_steps: List[str]
    
    # æ•°æ®æµ
    messages: List[BaseMessage]
    intermediate_results: Dict[str, any]
    final_result: Optional[Dict]
    
    # é”™è¯¯å¤„ç†
    errors: List[str]
    retry_count: int
    
    # é…ç½®
    config: Dict[str, any]
```

### å·¥ä½œæµèŠ‚ç‚¹å®šä¹‰
```python
# src/graph/nodes.py
from typing import Dict
from .workflow_state import WorkflowState

async def input_processing_node(state: WorkflowState) -> WorkflowState:
    """è¾“å…¥å¤„ç†èŠ‚ç‚¹"""
    user_input = state["user_input"]
    
    # è¾“å…¥æ¸…ç†å’ŒéªŒè¯
    cleaned_input = clean_user_input(user_input)
    
    # æ„å›¾è¯†åˆ«
    intent = await recognize_intent(cleaned_input)
    
    # æ›´æ–°çŠ¶æ€
    state["intermediate_results"]["intent"] = intent
    state["intermediate_results"]["cleaned_input"] = cleaned_input
    state["current_step"] = "input_processed"
    state["completed_steps"].append("input_processing")
    
    return state

async def agent_routing_node(state: WorkflowState) -> WorkflowState:
    """æ™ºèƒ½ä»£ç†è·¯ç”±èŠ‚ç‚¹"""
    intent = state["intermediate_results"]["intent"]
    
    # æ ¹æ®æ„å›¾é€‰æ‹©åˆé€‚çš„ä»£ç†
    agent_type = route_to_agent(intent)
    
    # è·å–ä»£ç†å®ä¾‹
    agent = get_agent_instance(agent_type)
    
    # æ‰§è¡Œä»£ç†ä»»åŠ¡
    agent_result = await agent.execute(
        task=state["intermediate_results"]["cleaned_input"],
        context=state["config"]
    )
    
    # æ›´æ–°çŠ¶æ€
    state["intermediate_results"]["agent_result"] = agent_result
    state["current_step"] = "agent_executed"
    state["completed_steps"].append("agent_routing")
    
    return state

async def result_formatting_node(state: WorkflowState) -> WorkflowState:
    """ç»“æœæ ¼å¼åŒ–èŠ‚ç‚¹"""
    agent_result = state["intermediate_results"]["agent_result"]
    
    # æ ¼å¼åŒ–ç»“æœ
    formatted_result = format_agent_result(agent_result)
    
    # ç”Ÿæˆç”¨æˆ·å‹å¥½çš„å›å¤
    user_response = generate_user_response(formatted_result)
    
    # æ›´æ–°æœ€ç»ˆç»“æœ
    state["final_result"] = {
        "response": user_response,
        "data": formatted_result,
        "metadata": {
            "processing_time": calculate_processing_time(state),
            "steps_completed": state["completed_steps"]
        }
    }
    
    state["current_step"] = "completed"
    state["completed_steps"].append("result_formatting")
    
    return state
```

### å·¥ä½œæµæ„å»º
```python
# src/graph/workflow.py
from langgraph import Graph
from .nodes import input_processing_node, agent_routing_node, result_formatting_node
from .workflow_state import WorkflowState

def create_fusionai_workflow() -> Graph:
    """åˆ›å»ºFusionAIä¸»å·¥ä½œæµ"""
    
    workflow = Graph()
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("input_processing", input_processing_node)
    workflow.add_node("agent_routing", agent_routing_node)
    workflow.add_node("result_formatting", result_formatting_node)
    
    # å®šä¹‰è¾¹å’Œæ¡ä»¶
    workflow.add_edge("input_processing", "agent_routing")
    workflow.add_edge("agent_routing", "result_formatting")
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("input_processing")
    
    # è®¾ç½®æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "agent_routing",
        should_continue,
        {
            "continue": "result_formatting",
            "retry": "agent_routing",
            "error": "error_handling"
        }
    )
    
    return workflow.compile()

def should_continue(state: WorkflowState) -> str:
    """åˆ¤æ–­å·¥ä½œæµæ˜¯å¦åº”è¯¥ç»§ç»­"""
    if state.get("errors"):
        if state["retry_count"] < 3:
            state["retry_count"] += 1
            return "retry"
        else:
            return "error"
    
    return "continue"
```

## ğŸ› ï¸ å·¥å…·é›†æˆæ¨¡å¼

### å·¥å…·è£…é¥°å™¨ç³»ç»Ÿ
```python
# src/utils/decorators.py
import asyncio
import logging
import time
from functools import wraps
from typing import Callable, Any

def ai_tool(name: str = None, description: str = None):
    """AIå·¥å…·è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # å·¥å…·æ‰§è¡Œå‰çš„å‡†å¤‡å·¥ä½œ
            start_time = time.time()
            logger = logging.getLogger(func.__module__)
            
            try:
                # æ‰§è¡Œå·¥å…·å‡½æ•°
                result = await func(*args, **kwargs)
                
                # è®°å½•æˆåŠŸæ‰§è¡Œ
                execution_time = time.time() - start_time
                logger.info(f"Tool {name or func.__name__} executed successfully in {execution_time:.2f}s")
                
                return {
                    "success": True,
                    "data": result,
                    "execution_time": execution_time,
                    "tool_name": name or func.__name__
                }
                
            except Exception as e:
                # è®°å½•é”™è¯¯
                execution_time = time.time() - start_time
                logger.error(f"Tool {name or func.__name__} failed: {str(e)}")
                
                return {
                    "success": False,
                    "error": str(e),
                    "execution_time": execution_time,
                    "tool_name": name or func.__name__
                }
        
        # æ·»åŠ å·¥å…·å…ƒæ•°æ®
        wrapper.tool_name = name or func.__name__
        wrapper.tool_description = description or func.__doc__
        wrapper.is_ai_tool = True
        
        return wrapper
    return decorator

def requires_api_key(service: str):
    """APIå¯†é’¥éªŒè¯è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # æ£€æŸ¥APIå¯†é’¥
            api_key = get_api_key(service)
            if not api_key:
                raise ValueError(f"Missing API key for {service}")
            
            # å°†APIå¯†é’¥æ·»åŠ åˆ°å‚æ•°ä¸­
            kwargs['api_key'] = api_key
            
            return await func(*args, **kwargs)
        
        return wrapper
    return decorator
```

### æ™ºèƒ½å·¥å…·é€‰æ‹©
```python
# src/agents/tool_selector.py
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer

class IntelligentToolSelector:
    """æ™ºèƒ½å·¥å…·é€‰æ‹©å™¨"""
    
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.tool_registry = {}
        self.tool_embeddings = {}
    
    def register_tool(self, tool_instance):
        """æ³¨å†Œå·¥å…·"""
        tool_name = tool_instance.tool_name
        tool_description = tool_instance.tool_description
        
        # å­˜å‚¨å·¥å…·å®ä¾‹
        self.tool_registry[tool_name] = tool_instance
        
        # è®¡ç®—å·¥å…·æè¿°çš„åµŒå…¥å‘é‡
        self.tool_embeddings[tool_name] = self.embedding_model.encode(tool_description)
    
    async def select_best_tool(self, task_description: str, context: Dict = None) -> Optional[str]:
        """æ ¹æ®ä»»åŠ¡æè¿°é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·"""
        if not self.tool_registry:
            return None
        
        # è®¡ç®—ä»»åŠ¡æè¿°çš„åµŒå…¥å‘é‡
        task_embedding = self.embedding_model.encode(task_description)
        
        # è®¡ç®—ä¸æ‰€æœ‰å·¥å…·çš„ç›¸ä¼¼åº¦
        similarities = {}
        for tool_name, tool_embedding in self.tool_embeddings.items():
            similarity = np.dot(task_embedding, tool_embedding) / (
                np.linalg.norm(task_embedding) * np.linalg.norm(tool_embedding)
            )
            similarities[tool_name] = similarity
        
        # é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„å·¥å…·
        best_tool = max(similarities, key=similarities.get)
        
        # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
        if similarities[best_tool] > 0.6:
            return best_tool
        
        return None
    
    async def get_tool_recommendations(self, task_description: str, top_k: int = 3) -> List[Dict]:
        """è·å–å·¥å…·æ¨èåˆ—è¡¨"""
        if not self.tool_registry:
            return []
        
        task_embedding = self.embedding_model.encode(task_description)
        
        recommendations = []
        for tool_name, tool_embedding in self.tool_embeddings.items():
            similarity = np.dot(task_embedding, tool_embedding) / (
                np.linalg.norm(task_embedding) * np.linalg.norm(tool_embedding)
            )
            
            recommendations.append({
                "tool_name": tool_name,
                "similarity": float(similarity),
                "description": self.tool_registry[tool_name].tool_description
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top_k
        recommendations.sort(key=lambda x: x["similarity"], reverse=True)
        return recommendations[:top_k]
```

## ğŸ”„ å¤šæ¨¡å‹åä½œæ¨¡å¼

### æ¨¡å‹é“¾å¼è°ƒç”¨
```python
# src/agents/model_chain.py
from typing import List, Dict, Any
import asyncio

class ModelChain:
    """æ¨¡å‹é“¾å¼è°ƒç”¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.models = {}
        self.chain_configs = {}
    
    def register_model(self, name: str, model_instance, capabilities: List[str]):
        """æ³¨å†Œæ¨¡å‹"""
        self.models[name] = {
            "instance": model_instance,
            "capabilities": capabilities
        }
    
    async def execute_chain(self, chain_name: str, initial_input: str, context: Dict = None) -> Dict:
        """æ‰§è¡Œæ¨¡å‹é“¾"""
        if chain_name not in self.chain_configs:
            raise ValueError(f"Chain {chain_name} not configured")
        
        chain_config = self.chain_configs[chain_name]
        current_input = initial_input
        results = []
        
        for step in chain_config["steps"]:
            model_name = step["model"]
            task_type = step["task_type"]
            
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not registered")
            
            model = self.models[model_name]["instance"]
            
            # æ‰§è¡Œå½“å‰æ­¥éª¤
            step_result = await self._execute_model_step(
                model, task_type, current_input, step.get("params", {})
            )
            
            results.append({
                "step": step["name"],
                "model": model_name,
                "result": step_result
            })
            
            # å‡†å¤‡ä¸‹ä¸€æ­¥çš„è¾“å…¥
            current_input = self._prepare_next_input(step_result, step.get("output_transform"))
        
        return {
            "chain_name": chain_name,
            "steps": results,
            "final_output": current_input
        }
    
    def configure_chain(self, name: str, steps: List[Dict]):
        """é…ç½®æ¨¡å‹é“¾"""
        self.chain_configs[name] = {
            "name": name,
            "steps": steps
        }
    
    async def _execute_model_step(self, model, task_type: str, input_data: str, params: Dict) -> Any:
        """æ‰§è¡Œå•ä¸ªæ¨¡å‹æ­¥éª¤"""
        if task_type == "chat":
            return await model.ainvoke([{"role": "user", "content": input_data}])
        elif task_type == "completion":
            return await model.agenerate([input_data])
        else:
            raise ValueError(f"Unsupported task type: {task_type}")
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

### AIè°ƒç”¨ç›‘æ§
```python
# src/utils/ai_monitor.py
import time
import psutil
from typing import Dict, List
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class AICallMetrics:
    """AIè°ƒç”¨æŒ‡æ ‡"""
    model_name: str
    start_time: float
    end_time: float
    input_tokens: int
    output_tokens: int
    cost: float
    memory_usage: float
    success: bool
    error_message: str = None

class AIPerformanceMonitor:
    """AIæ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics: List[AICallMetrics] = []
        self.model_stats = defaultdict(lambda: {
            "total_calls": 0,
            "total_cost": 0.0,
            "total_tokens": 0,
            "avg_response_time": 0.0,
            "error_rate": 0.0
        })
    
    def start_monitoring(self, model_name: str) -> Dict:
        """å¼€å§‹ç›‘æ§AIè°ƒç”¨"""
        return {
            "model_name": model_name,
            "start_time": time.time(),
            "start_memory": psutil.virtual_memory().used / 1024 / 1024  # MB
        }
    
    def end_monitoring(self, monitor_context: Dict, success: bool = True, 
                      input_tokens: int = 0, output_tokens: int = 0, 
                      cost: float = 0.0, error_message: str = None):
        """ç»“æŸç›‘æ§å¹¶è®°å½•æŒ‡æ ‡"""
        end_time = time.time()
        end_memory = psutil.virtual_memory().used / 1024 / 1024
        
        metrics = AICallMetrics(
            model_name=monitor_context["model_name"],
            start_time=monitor_context["start_time"],
            end_time=end_time,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cost=cost,
            memory_usage=end_memory - monitor_context["start_memory"],
            success=success,
            error_message=error_message
        )
        
        self.metrics.append(metrics)
        self._update_model_stats(metrics)
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "total_calls": len(self.metrics),
            "model_breakdown": dict(self.model_stats),
            "recent_metrics": self.metrics[-10:],  # æœ€è¿‘10æ¬¡è°ƒç”¨
            "system_health": self._get_system_health()
        }
    
    def _update_model_stats(self, metrics: AICallMetrics):
        """æ›´æ–°æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.model_stats[metrics.model_name]
        stats["total_calls"] += 1
        stats["total_cost"] += metrics.cost
        stats["total_tokens"] += metrics.input_tokens + metrics.output_tokens
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        response_time = metrics.end_time - metrics.start_time
        stats["avg_response_time"] = (
            (stats["avg_response_time"] * (stats["total_calls"] - 1) + response_time) / 
            stats["total_calls"]
        )
        
        # è®¡ç®—é”™è¯¯ç‡
        error_count = sum(1 for m in self.metrics 
                         if m.model_name == metrics.model_name and not m.success)
        stats["error_rate"] = error_count / stats["total_calls"]
```

## ğŸ”— ç›¸å…³è§„åˆ™æ–‡ä»¶
- [fusionai-overview.mdc](mdc:.cursor/rules/fusionai-overview.mdc): é¡¹ç›®æ•´ä½“æ¶æ„
- [langgraph-workflow.mdc](mdc:.cursor/rules/langgraph-workflow.mdc): LangGraphå·¥ä½œæµè¯¦è§£
- [tools-integration.mdc](mdc:.cursor/rules/tools-integration.mdc): å·¥å…·é›†æˆæŒ‡å—
- [performance-optimization.mdc](mdc:.cursor/rules/performance-optimization.mdc): æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
